\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Conference Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}

}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}

}

\maketitle

\begin{abstract}
\cite{pimm2015many}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
Identifying different species and types of plants growing in our natural habitat has long been an important task. However, these tasks have not been easy due to the abundance of plant species we have and the fact that many plants have a lot of similar characteristics. Therefore, experts in the field of agriculture have been trying to identify and document these plants correctly. But experts also find the task of identifying plant species difficult due to their abundance and characteristics.

With many plants on the verge of going into extinction and the biodiversity at risk \cite{pimm2015many}, there has been an increase in the need to reduce the task of experts in plant species conservations to be able to identify plants as easy and less time consuming as possible. From a farmer’s point of view, it is crucial to have a system in place that can differentiate between desired plants and unwanted plants - weeds. The automatic detection and controlling of weeds with herbicides using robotics will allow for precise farming, which will, in turn, increase the yields from a planting season \cite{bah2018deep}. This exact treatment for plants with what they need can reduce farmers’ investment, potentially leading to greater profits. Precise use of herbicides on detected weeds can also contribute to a reduction in the environmental pollutions caused by the traditional way of spraying herbicides evenly across farmland.

Recently, researchers have trialled the use of machine learning algorithms to identify plant species through leaf images. The use of this technique is possible since expert botanists often use leaf shape, colour and textures amongst other things for plant leaf classification task \cite{xiao2010hog, kadir2013leaf}. However, using computer vision which is a subset of machine learning, for this task has proven to be challenging \cite{kamilaris2018deep}. These challenges are due to the fact that the feature used for leaf identifications and classifications were handcrafted into these algorithms. Handcrafting these features meant that the algorithm did not learn how to do the classification job independently. Hence it is prone to the same errors expert botanists will make.

As a solution to these challenges with using computer vision for leaf image classification, the use of deep learning techniques have been explored and have shown tremendous results. The success of deep learning has been attributed to the advancement in the capabilities of graphics processing units (GPU), which can now perform the computationally expensive tasks of deep learning at incredible speeds. The essential advantage of deep learning over the other traditional machine learning techniques is its ability to automatically extract critical features from the input data using the characteristics of its deep neural networks. Several research studies have shown that convolutional neural networks (CNN) are best suitable for image classification jobs out of the deep learning neural network types.

On a positive note, deep learning models are currently helping millions of agricultural professionals and hobbyists in the correct plant species identification through mobile applications like Leafsnap and Pl@ntNet \cite{kumar2012leafsnap, get the one for plantvillage}.

Thanks to the capabilities of CNN, a new dawn has been set in making the task of plant recognition relatively easy and robust for interested individuals.

The rest of the paper is structured as follows.


\section{Literature Review}

\subsection{Maintaining the Integrity of the Specifications}


\section{Materials and Methods}


\subsection{Deep Learning}\label{AA}

In the past, image classifications workflows consist of processes like image pre-processing, feature extractions and finally classification with machine learning algorithms like support vector machine (SVM), random forest (RF), K-nearest neighbour (KNN), amongst others. The shape, colour, texture and vein were among the features extracted from the leaf images. Texture features were extracted using methods like local binary pattern, and Gabor filter  \cite{guo2010completed, li2010selection}. Shape and colour extraction techniques like scale space, discrete wavelet transform (DWT) and comparing the colour of images to predefined reference colours are used to extract these features \cite{satpute2016color}. However, the fact that there was no automation of this process made them impractical for broad adoption.

Currently, deep learning methods, particularly CNN, are used to classify plants using leaf images.


Deep learning is a model to learning for computers that have some of its bases on the understanding of how the human brain learns complex and straightforward things \cite{wang2017origin}.
Out of the subset of machine learning, deep learning is the most active field \cite{angermueller2016deep}.

\subsection{Convolutional Neural Networks}
Convolutional neural networks are a type of neural network in DL that is designed for processing grid-like or multi-array kinds of data \cite{lecun2015deep,goodfellow2016deep}. These kinds of neural networks typically consist of an input image, convolution, pooling and fully connected layers. The networks architecture of CNN allows them to train on deep layered data structures at a fast pace and robust in the correct classification of images \cite{nielsen2015neural}. The achievement of these capabilities are possible through the use of four essential components - the use of many layers, local connections, shared weights, and pooling \cite{lecun2015deep}.

\subsubsection{Convolutional layer}
This layer performs mathematical operations on the input image to extract the so-called feature map by using a filter - kernel \cite{goodfellow2016deep}. The feature map is obtained by summing results gathered from the multiplication of pixel by pixel value and kernel values as the function scans the image from the top left corner down to the bottom right of the picture. This operation results in a smaller size version of the input image. Figure 2 shows a representation of this operation.

A padding operation of the original pixel matrix is done to ensure that all pixels, mainly the corner pixels in the input image, participate in multiple convolution/feature detection operations. This is typically achieved by adding arbitrary numbers of pixels around the boundaries of the original image. For example, padding of two translates to adding two extra pixels around the borders of the original image matrix. The scanning of the image pixels is called stride, i.e. the number of pixels the filter moves over the input image pixels. For example, a stride value of 3 means that the filter moves by 3 pixels over the image.


\subsubsection{Pooling layer}
The pooling layer is the next layer after the convolution layer. The reduction of features extracted from convolutional operations performed in the preceding layer occurs in the pooling layer. Max pooling operation chooses the maximum value within a rectangular quarter while average pooling takes the average of the values in each area \cite{goodfellow2016deep}.
Achieving non-linearity in the learned networks is made possible by an activation function.
An activation function is a function that ensures non-linearity in the networks by determining the activation of neurons in the convolutional and pooling layer \cite{nielsen2015neural}. Rectified Linear Unit (ReLU) is the most commonly used activation function in CNN.



\subsubsection{Fully Connected Layer}
Recognition and classification of extracted data from previous layers occur in this layer. In this layer, the neurons direct connections to all activated neurons in the pooling layers. The outputs in this layer result from the direct connection to all activated neurons in the max pooled layer. Figure 2 shows an overview of the layers in a general CNN architecture.

This paper uses a pre-trained CNN architecture because it is a fact that characteristics learned in a pre-trained CNN model on large datasets can be fine-tuned for a totally new task \cite{}. Likewise, there is not enough diversity in the dataset for training a CNN model from scratch. Hence the reason for resolving to a pre-trained CNN architecture - AlexNet and ResNet. 


\subsection{CNN Architectures used (AlexNet, Resnet-34)}
The paper used the Alexnet and ResNet-34 architectures for training the model in the experiment.

Alexnet is a CNN architecture introduced by \cite{krizhevsky2012imagenet}. Alexnet architecture consists of 8 weight layers: 5 convolutional layers and 3 fully connected layers. The first, second and fifth convolutional layers are each followed by normalisation and a max-pooling layer. The first convolutional layer filters the input image with 96 kernels of 11 x 11 in size and a stride of 4 pixels. The second layer filters the input of the first convolutional layer with 256 kernels of size 5 x 5 and a stride of 1 pixel. The third, fourth, and fifth convolutional layers have 384, 256 and 384 kernels with sizes 3 x 3. The fully connected layers have 4,096 neurons each, and the RELU non-linearity function is applied to the output layers and that of convolutional layers.
The pre-trained network of Alexnet on the ImageNet dataset can be used to classify images into 1000 object categories.

Likewise, the Resnet-34 architecture is a CNN network with 34 deeper weight layers introduced by \cite{he2016deep}. The Resnet architecture turns a plain neural network (VGG nets but with fewer filters and lower complexity) into a residual network by inserting shortcut connections between the original network layers. The shortcut connections fix the vanishing gradient problem in the deep CNN network by creating an alternative bypass for the gradient by adding outputs from previous layers to that of the stack layers. Figure x shows the network architecture of Restnet in comparison with that of plain CNN.


\subsection{Dataset}
The paper performed the experiments using 2 standard datasets; Swedish leaf \cite{soderkvist2001computer} and AgrilPlant \cite{pawara2017comparing}.

\subsubsection{Swedish Leaf Dataset}
For plant recognition researches, \cite{soderkvist2001computer} published the Swedish leaf dataset. This dataset consists of 1125 leaf images from 15 different plant species classes. Thus, each plant species contains precisely 75 images. The tree classes are Ulmus carpinifolia, Acer, Salix aurita, Quercus, Alnus incana, Betula pubescens, Salix alba 'Sericea', Populus tremula, Ulmus glabra, Sorbus aucuparia, Salix sinerea, Populus, Tilia, Sorbus intermedia, Fagus silvatica. The leaf images of each plant species were in a laboratory on a white background. Figure x shows sample images from the Swedish leaf dataset.


\subsubsection{AgrilPlant Dataset}
 \cite{pawara2017comparing} introduced the AgrilPlant dataset for plant recognition tasks. This dataset consists of precisely 300 images of 10 classes of plant, amounting to a total of 3000 images. Flickr website is the source of the images in this dataset. The images may contain the landscape of the entire plant, branch, leaf, fruit and flower. The Agrilplant dataset is very challenging because of the following reasons:
 
 \begin{itemize}
     \item There are similarities among some classes, i.e. apple, orange, and persimmon, which have similar shapes and colours.
     \item There is a diversity of plants within the same class. For example, there are green and red apples, or there are varieties of tulips.
     \item The images contain varieties of objects in the background not relevant to the recognition task. These background noises are from the outdoor environments where the images were taken.
     \item There is a similarity among some classes. For example, apple, orange, and persimmon images consist of similar shapes and colours.

Figure x shows sample images from the AgrilPlant dataset.

 \end{itemize}

\subsection{Hardware and software}
The paper used Jupyter Notebooks \cite{kluyver2016jupyter} running Python 3 kernels with the PyTorch \cite{paszke2017automatic} and fast.ai \cite{howard2018fastai} libraries for training and evaluating the CNN models on a Linux server with 32 GB of RAM and a GTX 1080 GPU with 8 GB of RAM.

\section{Results}
will present the result without explaining why\\
will compare the result with other research papers\\

\section{Discussion}
Will explain the results\\
the shortcomings\\
advise for future research
\section{Conclusion}

\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{Some Common Mistakes}\label{SCM}

\subsection{Authors and Affiliations}


\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}


\bibliographystyle{plain}
\bibliography{references}
\vspace{12pt}


\end{document}
